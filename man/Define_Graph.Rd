% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AsynchronousAdvantageActorCritic.R
\name{Define_Graph}
\alias{Define_Graph}
\title{Graph for Network Loss according to A3C.}
\usage{
Define_Graph(model, model.par, game.object)
}
\arguments{
\item{model}{A Neural Network e.g. as given by \code{Setup.Neural.Network.A3c} or \code{Setup.Neural.Network.A3c.LSTM}.}

\item{model.par}{A list with parameters to set up the Network e.g. as given by \code{Get.Def.Par.Neural.Network.A3C} or \code{Get.Def.Par.Neural.Network.A3C.LSTM}.}

\item{game.object}{A Game Object (list) as defined by \code{Get.Game.Object.<NAME>}.}
}
\description{
Defines a tensorflow graph specifiying the loss calculation according to the A3C algorithm.
}
\details{
Returns a list of tensors with the following items: \itemize{
\item \strong{gradient.clipped} Calculated gradients of the network weights.
\item \strong{loss.policy} Calculated loss of the policy.
\item \strong{loss.value} Calculated value loss.
\item \strong{loss} Calculated total loss of the network.
\item \strong{s_} Placeholder for states.
\item \strong{a_} Placeholder for actions.
\item \strong{r_} Placeholder for rewards.
\item \strong{adv_} Placeholder for calculated advantages.
}
}
